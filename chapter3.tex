\chapter{Procedural Literacy: Toward a Programming Public}
\label{literacy}
\epigram{
  |Writing programs| had to do with using them to be a mirror of your thought process, to actually learn how to think.…I think everybody in this country should learn how to program a computer, should learn a computer language, because it teaches you how to think. It's like going to law school,…it teaches you how to think in a certain way.…I view computer science as a liberal art. It should be something that everybody learns. \autocite{Jobs1995}
}

\subsection*{Introduction}
In this chapter, I develop the concept of procedural literacy as a pedagogical facet of the ludocapitalist subject. As a key topic in public education policy debates and a rhetorical source of public funding for emerging, large-scale mass code literacy campaigns and technology-education reform movements, procedural literacy embodies an idealization of education most appropriate to ludocapitalism. Combining low-level operational knowledge of complex technological systems with a strategic, participative understanding of dynamic social environments, the concept of procedure is as equally applicable to "rules of the game" as to the functional composition of software artifacts. As a broad platform of technocultural pedagogy, procedural literacy thus purports to capture something essential about how ludocapitalism operates through social-technical procedures increasingly mediated by computing machines.

Though I support the basic progressive tenets of procedural literacy, I believe that its discourse so far has been mostly celebratory and largely lacking in critical analysis. My novel contribution to this discussion is to situate the concept within a more nuanced political-ethical context from which a critical perspective can be made possible. To accomplish this, in this chapter I supplement the contemporary technocultural emphasis on computer code with an orthogonal concept of procedure within legal theory, from a perspective which understands modern administrative systems of legal codes through the normative, idealizing assumptions of democratic principles derived from Enlightenment philosophy. Though my research in this direction is speculative, I believe that linking procedural literacy to a richer democratic context and raising questions about the legitimacy of knowledge on offer to the public is crucial if the contemporary deployment of mass procedural literacy is to become socially transformative and resist acting as a passive form of technical indoctrination in service of existing technocratic power.

My discussion within this chapter proceeds as follows:

First, I introduce contemporary discourses of procedural literacy within industry and academia, starting with Codecademy's industry-supported campaign for mass programming education, followed by academic discussions of procedural literacy by Murray, Mateas, and Bogost within the digital humanities field. While these two forms of procedural literacy advocacy are each at the forefront of articulating, validating and disseminating new forms of procedural knowledge embodied in software and videogame artifacts to broad publics, they both share the limitations of a dispassionate approach toward procedural knowledge that accepts the technical content of such forms of procedural literacy as they are given. In response, I draw a distinction between such mastery and a more nuanced critical literacy (drawing upon Turkle's distinction between "hard" and "soft" mastery), and argue that theories of procedural literacy require a stronger articulation of the latter through greater recognition of the links between the public adoption of specific bodies of linguistic-technical knowledge and the education ideals of democratic society.

Next, I elaborate the theoretical basis of a social-political understanding of procedural literacy by interrogating the link between rhetorics of literacy and the Habermasian ideal of a bourgeois public sphere. Noting that this link has been more developed for the legal concept of "procedure" than for computer code, I develop a critical theory of procedural literacy by relating Habermas's theory of the public sphere within a proceduralist paradigm of law to the computing context.

Third, following this theoretical discussion I develop a practice-oriented critique of programming language systems that considers specific computing languages in terms of the legitimacy of their notational systems to be informed by and held accountable to a broad public sphere. I analyze and compare two alternative literacy-oriented programming language frameworks, DrScheme and Processing, focusing my observations on the contributions and limitations each framework offers toward a broad procedural literacy.

Finally, I conclude by summarizing the practical insights derived from these programming framework critiques into generative tactics for critical procedural literacy practice.

\section{Procedural Literacy}
In this section, I examine the concept of procedural literacy as it has developed in contemporary technoculture as a particular perspective on modern computer programming education. I understand this term as the intersection of procedure, a concept with roots in modern bureaucratic and judicial administration describing the mechanization or routinization of illocutionary (or "effective") action, with literacy, a socio-political condition produced by the production and dissemination of a standardized forms of knowledge, technologies, and educational institutions for a mass public.

The notion of procedure has a history within legal formalism predating that of the electronic computer. Weber understood the concept of procedure as the link between the rationalization of modern administrative machinery and the formalization of legal procedure:
\blockquote{
  Juridical formalism enables the legal system to operate like a technically rational machine. Thus it guarantees to individuals and groups within the system a relative maximum of freedom, and greatly increases for them the possibility of predicting the legal consequences of their actions. Procedure becomes a specific type of pacified contest, bound to fixed and inviolable >rules of the game.< \autocite*[811]{Weber1978}
}
Weber's use of a game-playing metaphor to illustrate this point about juridical formalism indicates that game rules had been recognized as a paradigmatic competitive procedural context well before the advent of economic game theory.

Likewise, the concept of the computing machine condensed and formalized the notion of procedure, established through a metaphorical comparison to the routine, bureaucratic performance of following such "rules of the game." Turing famously formalized the concept of the "computing machine" as the mechanical idealization of a well-disciplined human agent manipulating arbitrary symbols with a pencil and paper: "We may compare a man in the process of computing a real number to a machine" \autocite*[231]{Turing1936}. In fact, Turing even explicitly invoked the bureaucratic concept of "rules of procedure" in a later description of his famous comparison: "It is possible to produce the effect of a computing machine by writing down a set of rules of procedure and asking a man to carry them out.…A man provided with paper, pencil, and rubber, and subject to strict discipline, is in effect a universal machine" \autocite*[416]{Turing1948}.

In recent years this mechanized concept of procedure, at the foundation of computing culture and other rationalist systems theories, has become a key term in debates over the expanding role of computer technology in liberal arts education. An increasing number of voices from both the tech industry and the digital humanities argue that the computerization of society has elevated procedural knowledge into a mass literacy, one that demands new approaches to computing education (and innovative educational technology) in order to best serve future generations of students. I will examine representative voices from industry and academia in turn.

\subsection*{Code Year}
In January 2012, an unusual New Year's resolution topped the lists of many tech-savvy bloggers, social media users, and other aspirational members of the digital periphery: learn to "code." This peculiar resolution was evangelized by Codecademy, a two-man startup company just four months old in its social media marketing campaign for "Code Year," a free e-mail subscription to weekly JavaScript web-programming tutorials delivered through an interactive web application designed for anyone, with no prior programming knowledge or experience required \autocite{Wortham12}. Backed by the prestigious and well-connected startup incubator Y Combinator, Code Year struck a cultural nerve and became a media phenomenon: Within a month, over 350 thousand people had subscribed to the campaign, most notably following an enthusiastic Twitter announcement of support from New York City billionaire mayor Michael Bloomberg. The startup secured over \$12 million in private venture capital funding from a group of international investors before the company's first anniversary, with hopes of growing their online tutorials into a global, profitable operation.

Codecademy represents a common, widely-criticized model of technical education as a commercial, global commodity, promising an efficient, guided drilling of programming syntax and grammar with little regard to context or application, branding itself as an easy, accessible, no-cost path to Internet enlightenment. The broad, vocal support for Code Year reflects the long-accepted insight that new digital literacies are facilitated by renewing cycles of information and communication technologies (ICTs), a quite commonplace position in public policy and education discourse in recent decades.\footnote{
  One notable example is the 2008 report of the \citeauthor{Knight09}, which offered a recommendation to "Integrate digital and media literacy as critical elements of education at all levels through collaboration among federal, state, and local education officials" \autocite[45]{Knight09}.
} The open, pragmatic question of how best to integrate computer technology into literacy has produced volumes of research on themes such as "technological," "digital," "new media," and countless other forms of literacy.\footnote{
  See for example \autocites{Dakers2006}{Jones2005}.
} Such far-reaching institutional debates concerning the use of computing technology in education provide a broad literacy context for a more narrowly-focused claim advanced by mass programming education ventures such as Codecademy, which I will focus on in this chapter: computer programming itself is becoming increasingly legitimated as a more fundamental form of literacy to be placed alongside print, visual, and other multi-modal literacies. The claim resulting from this strong form of code literacy is that everyone, not only computer scientists or software industry professionals, can and should learn to code, not merely for vocational purposes but to "actually learn how to think," as Apple founder Steve Jobs famously described the fundamental purpose of programming in a 1995 interview \autocite*{Jobs1995}.

Beyond the insular world of tech startups and venture capital, rallies for mass programming literacy such as Code Year have ignited a lingering debate in the academic field of digital humanities regarding the social and pedagogical value of cultivating programming skills outside software engineering professions.\footnote{
  For an overview of academic responses to Codecademy, see \autocites{Widner12}; see also \autocites{Williamson12} for a more critical overview.
}
Although many code-savvy digital humanists and technologists agree that some amount of fluency in the vast, rapidly-evolving array of technical languages and computing jargons developed within the software industry is increasingly essential for contemporary knowledge work, there is an acute tension within the humanities between linguistic-technical mastery and broader social-cultural context. It is within this tension that the concept of procedural literacy has developed, expanding to encompass not merely the technological environments of computing machines but also other rule-based procedural systems that circumscribe contemporary society, including legal codes, economic markets, games and simulations.

\subsection*{Procedural Literacy}
Within the contemporary academic field of digital humanities, the principal property of the computer as embodying the essence of bureaucratic "procedure" was rediscovered in 1997 by Murray, who argued that "the procedural power of the computer" lay in "its defining ability to execute a series of rules" \autocite*[71]{Murray1997}, and looked forward to future forms of "procedural authorship" and "procedural virtuosity" that would better suit the new medium. Michael \citeauthor{Mateas05}, drawing upon experience teaching undergraduate new media courses to non-computer science majors, built upon Murray's concept in \citeyear{Mateas05} to define "procedural literacy" broadly as "the ability to read and write processes, to engage procedural representation and aesthetics, to understand the interplay between the culturally-embedded practice of human meaning-making and technically-mediated processes" \autocite*[101--2]{Mateas05}. With this somewhat circular definition, Mateas took pains to avoid any hasty reduction of the concept to any easily-instrumentalized programming skill: although "the craft skill of programming is a fundamental component of procedural literacy, it is not the details of any particular programming language that matters, but rather the more general tropes and structures that cut across all languages" \autocite[102]{Mateas05}. As a result, this academic concept of procedural literacy has gone beyond competing tech industry efforts to distance itself from a purely vocational or instrumental notion of technocultural knowledge.

I read Mateas's and Murray's standard models of procedural literacy as the contemporary revival of an education concept with a history almost as long as that of the electronic computer itself. To illustrate this lineage, I will next compare this standard contemporary model against historical versions of procedural literacy offered in the early 1960s and again in the early 1980s. Following this discussion, I will consider Bogost's variation on this procedural literacy model that attempts to address issues with its ahistoricity and universality.

\subsubsection*{History of Procedural Literacy}
As early as 1960, forward-thinking academics in the earliest computer-equipped universities have been calling for an increased presence of computer programming activity in liberal arts education. In a 1960 lecture series in celebration of MIT's centennial on the theme "Management and the Computer of the Future," computer scientist Alan \citeauthor{Perlis62} delivered a talk on \citetitle{Perlis62} where he outlined his vision of the computer's critical role in modern education, urging its inclusion in an undergraduate curriculum:

\blockcquote[187--8]{Perlis62}{
  |T|he product of a university education should receive training directed to the development of sensitivity, rationality, and an intelligent table look-up procedure.
  
  Sensitivity,…is a feeling for the meaning and relevance of facts. Rationality is fluency in the definition, manipulation, and communication of convenient structures, experience and ability in choosing representations for the study of models, and self-assurance in the ability to work with the large systems that are unfortunately necessary for modeling and solving the important problems of our times. Table look-up, of course, refers to the mechanism for gaining access to a catalog of facts and problems that give meaning and physical reference to each man's concept of, and role in, society. While the computer may conceivably play a small role in the development of human sensitivity\footnote{
    In response to a comment from J. C. R. Licklider, Perlis later removes his reservation on this point, agreeing that the computer can play a critical role in developing sensitivity as well \autocite*[204]{Perlis62}.
  }, it is quite critical to the other two developments.…
  
  |T|he first student contact with the computer should be at the earliest time possible: in the student's freshman year.…|E|ach student during this first course should program and run or have run for him a large number of problems on the computer. At least in engineering and science programs, this course should share with mathematics and English the responsibility for developing an operation literacy, while physics and chemistry develop the background toward which this literacy is applied. In a liberal arts program the course could be delayed until the sophomore year, but certainly deserves inclusion in such a program because of the universal relevance of computers to our time.
}
In this talk, delivered at a time when less than a hundred electronic computers existed in the world, Perlis raised the nascent discipline of computer science to the same level of broad public significance as mathematics or literature, advocating a shared responsibility for developing an "operation literacy" he saw as crucial for the "fluency" or "rationality" required to work with the "large systems" tasked with "modeling and solving the important problems of our times." Perlis's call for code literacy was marked by an enthusiastic acceptance of the overwhelming complexity underlying modern society's machinery, a computational rationality both "unfortunately necessary" and with "universal relevance" in the postwar cybernetic era.

This vision met with general approval from its audience, comprised of many influential figures in the emerging academic-military-industrial computing complex. However, two important, contrasting visions were also presented in the ensuing discussion, which I believe represent important critical responses to Perlis's increasingly popular advocacy for "operation literacy." First, Peter Elias commented with an alternative vision of a much more restricted role for computer programming education in future generations:
\blockcquote[qtd.~in][203]{Perlis62}{
  I have a feeling that if over the next ten years we train a third of our undergraduates at M.I.T. in programming, this will generate enough worthwhile languages for us to be able to stop, and that succeeding undergraduates will face the console with such a natural keyboard and such a natural language that there will be very little left, if anything, to the teaching of programming.
}
While Elias's feeling that the challenge of constructing programming languages as expressive as natural language would be completed within a decade was ultimately shortsighted, the general idealism in his statement remains an open, provocative question: Does the technical gap between programming languages and "natural" languages used in everyday forms of communication represent the emergence of a fundamentally new paradigm of knowledge at the heart of human rationality, or is it instead a temporary technical limitation, one that can be delegated to a specialized technical discipline and reduced or eliminated with greater research and development efforts into machine-language design? If the latter, then designing programming languages to be easier to use, and to be integrated as closely as possible with its audiences' existing "natural" language development, could be even more beneficial to the development of procedural literacy than a focus on training or teaching methods alone.

Second, the historian of science C.P. Snow (most famous for his "Two Cultures" lecture) delivered a lecture in the same series that related an ongoing concern about an emerging technocratic order surrounding computing machines, envisioning a scenario where "a handful of people, having no relation to the will of the society, having no communication with the rest of society will be taking decisions in secret which are going to affect our lives in the deepest sense" \autocite*[9]{Snow62}. In contrast to Perlis's embracing the complexity of monolithic computing machines and the need to train students to operate them (the exorbitant cost and exclusive access giving this literacy theme an elite, exclusive character) as an inevitability of the modern age, Snow was concerned by the tendency of such complex systems to consolidate power in the hands of an elite few, leaving those without privileged access to the relevant decision-making context subject to the interests of those in power, and vulnerable to systematic exploitation and structural inequality.

Snow's concern, acutely felt amidst the wealth of secretive, expensive, expert-oriented government-funded defense and operations research taking place at the time, illuminated a key issue at stake in procedural literacy: technical languages, conceptual models, and even methods of posing problems and forming solutions produced within restricted environments have a tendency to reproduce their designers' enclosed ideologies. When such a technical code starts to expand into a mass literacy, while beneficial to industries or individuals ahead of the curve and equipped with the relevant expertise, it also subjects this expanded public to a structural dependency upon the codified knowledge and institutions responsible for its standardization and governance. Sometimes this subjection is explicit, as is the case for proprietary programming languages or operating systems where the adoption and use of such technologies remains commercialized and dictated by the constraints of intellectual property licensing agreements. However, as \citeauthor{Bryson1994} argue, it is precisely when technological paradigms begin to take root in educational contexts as "implicit, embedded stories" to the exclusion of other possibilities and prospective uses that "educational technologies can become \emph{technologies of normalization}" \autocite[217]{Bryson1994}.

Indeed, a common theme of such technologies of normalization is the refrain that the form of knowledge language on offer is not merely educational for the particular affordances it might offer the learner, but that it provides a universal, abstract, rational competency. Perlis articulates this dialectic of programming as deriving a universal literacy from a particular practice: "The point is not to teach the students how to use Algol, or how to program the 704. These are of little direct value. The point is to make the students construct complex processes out of simple ones (and this is always present in programming), in the hope that the basic concepts and abilities will rub off. A properly designed programming course will develop these abilities better than any other course" \autocite[206]{Perlis62}. Mateas, in his own account of Perlis's lecture, reads this argument approvingly: "Here Perlis makes it clear that programming is a medium, in fact \emph{the medium} peculiarly suited for describing processes, and as such, a fundamental component of cultural literacy, and a fundamental skill required of new media practitioners and theorists" \autocite*[7]{Mateas05}.

The concept of procedural literacy as a neutral, universal signifier of "basic concepts and abilities," of programming as "\emph{the medium} peculiarly suited for describing processes" is an image that links the mainframe machines of the 1960s as the subject of Perlis's literacy to the ubiquitous, networked mass computing environments of the 2000s that concerned contemporary digital humanists such as Mateas. Spanning these two generations, Seymour Papert's decades-long affiliation with the Logo project is perhaps the most influential and popular technology education project in the half-century history of universal procedural literacy discourse. Logo originally began as an NSF-funded research project to develop techniques for teaching children formal mathematical concepts using computers, starting in the mid-1960s at the technology firm Bolt, Beranek and Newman, with several core researchers including Papert continuing development at MIT through the 1970s.\footnote{ For a good summary of Logo's early project history, see \autocite{Logohistory1999}.} In the early 1980s, as personal computers were just beginning to enter schools and households, \citeauthor{Papert80}'s widely popular book \citetitle{Papert80} sparked an educational movement around Logo, which had assumed a broader significance as "the name of a philosophy of education in a growing family of computer languages that goes with it" \autocite[217]{Papert80}. In this book, Papert explicitly linked the computing machine to the universal rationality of abstract human thought: "The computer is the Proteus of machines. Its essence is its universality, its power to simulate" \autocite[viii]{Papert80}; "My interest is in universal issues of how people think and how they learn to think" \autocite[10]{Papert80}. His work is also an early source of an intentional ambiguity in the word "procedure," simultaneously referring to the restricted concept of procedure used within computer science (a program subroutine or a named sequence of machine-executable commands) and the generalized concept of a rule-based social activity. He describes the latter as a "people procedure," presenting one example where he constructs a pseudo-Logo code sequence of fine-motor actions that a human juggler might invoke to execute the people procedure "TO JUGGLE" \autocite[107]{Papert80}. Starting from the Logo-language computer-based microworld and expanding outward into the real world in this way, Papert argues that the computer is a uniquely appropriate place for children to develop the universal quality of "procedural thinking," faintly echoing Weber's own game-playing metaphor:
\blockcquote[154]{Papert80}{
  Why don't children learn a procedural approach from daily life? Everyone works with procedures in everyday life. Playing a game or giving directions to a lost motorist are exercises in procedural thinking. But in everyday life procedures are lived and used, they are not necessarily reflected on. In the LOGO environment, a procedure becomes a thing that is named, manipulated, and recognized as the children come to acquire the idea of procedure.
}
Despite Papert's stated intention that his focus is "not on the machine but on the mind," and that he sees the computer as a mere "carrier of cultural >germs< or >seeds< whose intellectual products will not need technological support once they take root in an actively growing mind" \autocite[9]{Papert80}, his argument for a universal procedural literacy rested on the assumption that the concrete "idea of procedure" as advanced by the Logo environment unproblematically corresponds with an abstract, universal mode of "procedural thinking" that operates invariantly throughout culture and history.

In response to such historical claims to the universality of computing and of procedural literacy that mirror contemporary ones, I find it helpful to recall that despite claims to be teaching broader concepts, each generation of procedural literacy was bounded by historical and material constraints regarding the specific content and intended goals of such computer-programming instruction. For example, Perlis's call to code in the 1960s addressed a sizable, though still relatively marginal, specialized labor force in service of substantially government-funded military and academic research interests. As Nathan \citeauthor{Ensmenger2010} recounts in his social history of computer programming, as computer applications began to enter the private sector a "software crisis" was identified in 1968, provoking several conflicting visions of the development of programming labor into a standardized commercial industry. The debates surrounding this crisis eventually consolidated on the professional-association model of "software engineering" comprised of skilled, high-tech workers more familiar to us today.

In contrast to both periods, contemporary calls to code reach beyond an elite few with privileged access to restricted and expensive university, military or corporate hardware installations, and even beyond a professional class of software engineers. Rather, they address a mass, middle-class, global public, one that views the acquisition of high-tech skills as a form of upward mobility, linked through the distributed communication spaces of the Internet with many orders of magnitude more powerful, portable and affordable computing resources at its disposal. With less expensive, mass-produced personal computer hardware reducing barriers of cost and access to within reach of a middle class consumers, programming has now become "something any high school student can do with a decent paperback on the subject and a couple of weeks of effort" \autocite[143]{Rushkoff2010}, suggesting that it can be delivered as mass education and productized into an educational commodity as in the Codecademy model.

\subsection*{Expanding Procedurality}
Keeping in mind this insight that computing has a concrete history that should not be abstracted into an ahistorical model of procedural literacy as a universal "fundamental skill," I next turn to Bogost's critical expansion of the concept of procedural literacy into a theory with greater cultural resonance. In \citetitle{Bogost2007-ow}, \citeauthor{Bogost2007-ow} supports and extends the concepts of procedural authorship and procedural literacy developed by Murray and Mateas, offering his own take on procedural literacy that emphasizes the craft-oriented construction of code. Like Papert, Bogost explicitly generalizes procedural literacy not just beyond any particular programming language but also beyond the computer itself:

\blockcquote[245--6]{Bogost2007-ow}{
  |P|rocedural literacy entails the ability to reconfigure concepts and rules to understand and [sic] processes, not just on the computer, but in general.…Procedural literacy should not be limited to the abstract ability to understand procedural representations of cultural values. Rather, it should use such an understanding to interrogate, critique, and use specific representations of specific real or imagined processes.}
\blockcquote[258]{Bogost2007-ow}{
  Procedural literacy entails the ability to read and write procedural rhetorics---to craft and understand arguments mounted through unit operations represented in code. The type of >reading< and >writing< that form procedural rhetorics asks the following questions: What are the rules of the system? What is the significance of these rules (over other rules)? What claims about the world do these rules make? How do I respond to those claims?
}
In addition, Bogost suggests that procedural literacy is a kind of learning that is fundamental to, or is at least privileged in, contemporary videogames, inviting comparisons to James Paul Gee's influential work on links between videogames and literacy and learning:

\begin{quoting}
  |T|he learning that takes place in videogames is not just comprised of abstract processes, following the constructivist tradition, nor their surface content, following the behaviorist tradition. Rather, videogames use abstract processes to make procedural claims about specific topics. \autocite[245]{Bogost2007-ow}
\end{quoting}

\begin{quoting}
  Videogame players develop procedural literacy through interacting with the abstract models of specific real or imagined processes presented in the games they play. Videogames teach biased perspectives about how things work. And the way they teach such perspectives is through procedural rhetorics, which players >read< through direct engagement and criticism. \autocite[260]{Bogost2007-ow}
\end{quoting}
Bogost's position displays a balanced pragmatism and a careful consideration of the previous theories of literacy upon which it builds, and as such it provides a good point of departure from which to situate my own critical contributions to the concept throughout the rest of this chapter. Its central insight is a critique of Papert's constructivist tradition for its focus on "abstract processes" as the ultimate end of procedural literacy, as opposed to any situated or domain-specific knowledge: "It is precisely specific areas of experience that have been expunged from our understanding of constructivist learning and procedural literacy in particular" \autocite[250]{Bogost2007-ow}. Bogost illustrates his argument by way of comparison to neoclassical approaches to the use of Latin as a cornerstone of literacy, from which he cites an influential text arguing that "Latin trains the mind to think in an orderly fashion. Latin,…is the mental equivalent of a daily two-mile jog" \autocite[Wise and Bauer, qtd.~in][248]{Bogost2007-ow}. In response, Bogost argues that the specific material and historical context of particular domains of knowledge negate the possibility of an abstract, universal literacy, because such learning is intricately tied to its context. Comparing neoclassical and procedural approaches to literacy, Bogost writes:
\blockcquote[249]{Bogost2007-ow}{
  Latin, |the programming language| C, and other language systems share basic properties.…|They| thus enforce a procedural rhetoric in each of their created artifacts.…But the cultural, historical, and material contexts for Latin and C are far from similar. Mastering the syntax and grammar of one over the other both opens up and closes down whole worlds of future knowledge and expression
}
Bogost's critiques both code-literacy rhetoric that focuses too strongly on acquiring programming skill, and the constructivist tradition's model of a universal procedural literacy. His argument for a for a procedural literacy that "reconciles subject-specificity and abstraction" \autocite[244]{Bogost2007-ow} is an important corrective that makes the concept more suitable for the humanities.

I base my departure from Bogost's position upon two central criticisms. First, I worry that this concept of procedural literacy becomes too generalized as a result of its middle-ground approach, to the point that it is offered as an abstract tool of rhetorical analysis arbitrarily discoverable in any system whatsoever without any means of evaluation. In one example, Bogost cites Playmobil toys as an example of procedural literacy qualitatively different from the kind he finds in Lego-construction play because the "high specificity" built into the Playmobil sets with themes such as metermaids and chimney sweeps "offers procedural learning on a much more deeply culturally embedded level than Lego" \autocite[256]{Bogost2007-ow}.\footnote{
  See also \autocite{BogostPlaymobil}.
 } In another example, he identifies the basic act of commodity consumption (as observed in the videogame Animal Crossing) as a "procedural rhetoric of debt and consumption": "Learning how to smartly amass and expend capital is a type of literacy that haunts many adults" \autocite[268]{Bogost2007-ow}.\footnote{
  An earlier published draft of the same essay makes this claim more explicit: "the process of amassing capital and then choosing how to expend it is a kind of procedural literacy that continues to haunt many adults" \autocite[36]{Bogost2005}.
}
By expanding the concept of procedure to encompass his particular variant of social systems theory, Bogost thus risks sweeping the totality of cultural activity, from highly specific forms of practice demanded of professional game designers, politicians and tax attorneys to more mundane forms of consumer purchasing and children's make-believe toys, into a precariously flat ontology under a universal heading of "procedure" encompassing the entirety of modern life.

Second, although Bogost takes pains to apply procedural literacy broadly to "processes, not just on the computer, but in general," its concrete application to videogames and software-engineering domains as object-oriented programming dominate the discussion at the expense of marginalizing other historical or alternative forms of bureaucratic procedure. This marginalization mirrors Papert's earlier subordination of "people procedures" to the computer-procedural perspective of Logo subroutines.

This combination of advocacy for such a relativistic notion of literacy with a self-conscious focus on market-dominant popular cultural genres and technological platforms risks implicitly naturalizing dominant forms of media technology that surround us, and is exacerbated by a rhetorical move that universalizes the concrete literacy in question to represent, in theory at least, any "system" whatsoever. This is a non-politics of separate-but-equal ontological egalitarianism that Bogost articulates in his later philosophical work: "All things equally exist, yet they do not exist equally" \autocite{BogostMaterialisms}.\footnote{ See also \citeauthor{Bryant2011}'s exegesis of Bogost's thesis in \citetitle{Bryant2011} \autocite[279--90]{Bryant2011}.}

In this view, all forms of play and procedure may "equally exist" as potential literacies but, much like contemporary "standard English" in its hegemonic relation to minor languages and dialects, popular, dominant, or "effective" literacies are viewed as more in touch with centralized flows of power in society and therefore become the default, prototypical instantiations worthy of sustained attention to the exclusion of others in educational environments.

On the one hand, such a pluralist embracing a totality of social, cultural and technical configurations into a concept of "procedure" productively resists both a technocratic reduction of the world into an overdetermined process dictated by the constraints of the machine as in programming-skill oriented education efforts, and also a conservative notion of literacy focused exclusively on print-based forms of communication. At face value, such a generalized concept of procedural literacy (here, I would also include the parallel concept of "multiple literacies" underlying Gee's own link between video games and literacy) particularly attentive to the technical complexities introduced by modern computer systems is a hard position to contest---after all, who can object to more, as opposed to less, familiarity and facility with the technical codes, processes and systems increasingly underpinning computerized society?

On the other hand, although the generalized procedural literacy concept developed by Murray, Mateas and Bogost has been rhetorically useful in breaking down lingering prejudices against unconventional or non-traditional literacies, I find it lacking any prescriptive or normative substance. There are many conflicting opinions regarding the desired degree and specific content of procedural competency publics at large should ideally be responsible for, particularly if such considerations are extended beyond individual preferences (such as Lego vs Playmobil play) to matters of public policy and political choices, prioritization of public education funding among competing literacy programs and interests, and so on. Should we build our complex computing infrastructure around the responsibility of a select few engineers and technical experts as Elias advocated, or should we exhort a broader mass society to learn general-purpose coding skills in order to cultivate a more code-literate public? If the latter, which programming languages or systems should we teach such publics, and on what basis should we select or develop suitable pedagogical material?

These are complex questions that a neutral, sociological embrace of all forms of procedural or technological literacy is not equipped to adjudicate, as the stakes and interests of designers, marketers, owners of and investors in the specific computing technologies on offer are too central to dispassionately ignore. In a critical survey of national funding of technological literacy programs in the United States, Cynthia \citeauthor{Selfe1999} notes that "on a pragmatic level, definitions of literacy serve as triggers, or requirements, for other socially determined systems of support," and "play a significant role in creating and maintaining a cohesive hegemonic system in the United States that affects every citizen's chances of success" \autocite[18]{Selfe1999}. Such rhetorics of digital literacy thus serve a normalizing function, legitimizing particular programming languages and environments, development and design paradigms over others by leveraging power and influence from educational institutions adopting their message. One classic example of such a hegemonic effect in Apple Computer's educational initiatives in the early 1980s, where the company's early dominance of the burgeoning personal computer market can be at least partially attributed to its strategic partnership with Papert's Logo Computer Systems, Inc. (LCSI), including a successful bid to donate one Apple II computer (bundled with an Apple-branded, licensed copy of LCSI Apple Logo) to every public and private school in the state of California, in exchange for tax breaks and a practically-assured market share \autocite{Blakeslee84}.

Accounts of procedural literacy are not blind to such associations between educational and software-industry organizations, as specific political-ethical invocations of literacy are implicated within such developments. What is at stake in the growing public interest in procedural literacy is the recasting of the post-Fordist landscape of computing power as a legitimate form of codified knowledge. From this perspective, the critical question is not whether any material practice, procedural system or artifact such as a programming language or a video game effectively or essentially constitutes a literacy for its subjects, but what specific constructions of literacy will further specific collective political and social goals, and how those goals are deliberated and articulated. I agree with Annette \citeauthor{Vee2013-bh}'s recent proposal that "a determination of whether or not a system of skills is a literacy depends on its societal context. One can be skilled at leveraging specific technologies to communicate, but a literacy leverages infrastructural symbolic technologies and is necessary for everyday life" \autocite[45]{Vee2013-bh}. As procedural literacy continues its transition from a specialist material technology to a necessity for everyday life, the question becomes how the selective legitimation of specific forms of procedural literacy support or contradict the political and ethical goals confronting variously-constituted programming publics. 

I noted the cultural resonance of Code Year as an example of the newly public revitalization of interest surrounding the potential for programming languages to become mass media of public expression, to identify the changing stakes in public procedural literacy rhetoric as a result of its increasing public participation. Within the idea of a mass literacy built around a practice of programming increasingly occupying a public stage, I find indications of broader political and ethical images of a better society, and various visions of a mass, middle-class, code-literate public that can distribute computing power and its attendant upward mobility in a purportedly democratic fashion. However, even within the academic wing of procedural literacy discourse emerging within the past decade, such political and ethical images and assumptions are rarely made an explicit facet of the concrete technologies on offer. Often, as in Bogost's work, any specific political orientation is actively effaced in favor of a systems-theoretical relativism. This is the gap I will attempt to address throughout the rest of this chapter. What ethical assumptions are involved in applying the rhetoric of literacy to specific forms of technical knowledge? What kind of public discourse is formed or imagined by a code-literate community, and what material work and social organization does it entail? How do such public calls for code literacy mediate between calls for the democratization of computer technology and theories of computerized democracy?

\subsection*{From Mastery to Literacy}
As a first step toward addressing such questions through my own contributions toward a critical code literacy derived from a procedure-oriented reading of the idealized concept of the public sphere, I will first distinguish a cognitive-performative, operational notion of \emph{mastery} from the ethical-political concept of a public-oriented \emph{literacy} that is the richer target for humanities and public policy fields, in order to focus more precisely on the unique aspects of the latter in subsequent sections.

The form of knowledge I identify with the concept of "mastery" is an operational form, conceivable as a symbolic machine. A fixed body of knowledge is presented through a system of notations (a code), and the objective is to achieve operational autonomy by mastering, or internalizing, the system of signs to the point where its complexity is regulated entirely under the operator's control. Such "operational knowledge" is not necessarily a simple commodity to be passively consumed, acquired and consciously re-deployed; it can represent a more complex system to be both actively reverse-engineered and perfected as a skill to be exercised fluently, as an unconscious intuition or situational awareness. Depending on the resources already available for mapping the system's internal structure to the subject's existing background knowledge of spatiotemporal metaphor, this learning process may involve more or less experimentation, predictive modeling, or practice.

Within cybernetic systems theory, this concept of mastery in the form of regulation and control has been formalized and generalized by Ashby, and is known as the law of "Requisite Variety." In \citetitle{Ashby1956}, \citeauthor{Ashby1956} introduces this law by way of a game-theoretic example: "suppose that we are watching two players, R |Regulator| and D |Disturbance|, who are engaged in a game" \autocite[202]{Ashby1956}. The game is based on a grid of outcomes, visible to both players, where D selects a row followed by R selecting a column. Ashby goes on to prove that "If no two elements in the same column are equal, and if a set of outcomes is selected by R, one from each row, and if the table has r rows and c columns, then the variety in the selected outcomes cannot be fewer than r/c" \autocite[206]{Ashby1956}. In more general terms, "only variety in R can force down the variety due to D; variety can destroy variety" \autocite[207]{Ashby1956}.

I mention Ashby's cybernetic principle only in order to reaffirm that mastery is not necessarily simple; it can represent knowledge of exceeding complexity, as long as the subject has sufficient >variety< to regulate the system's complexity. This form of knowledge accepts the system's logical rules and formalisms as a given structure, assuming the goal of incorporating the system's complexity into one's own subjectivity, steering future actions and interpretations towards the elimination of any uncertainty and the exercise of complete control of the system. Like practicing a tennis backhand, the conditioning of such mastery through training exercises can appear as a sort of empowering repetition. Like levels of play attained by expert chess grandmasters, the pursuit of mastery may even draw upon creative or intuitive human faculties of pattern recognition or linguistic processing inaccessible to self-conscious reflection.

Subjecting one's linguistic performance to the norms of a higher structure, through mastery of its form one learns to exercise control over the system and thus partake in the sovereign power the machine promises. The classification of mastery enables and encourages the formation of identities, the construction of boundaries, and systems of inclusion/exclusion based on the recognition of difference---between the master and slave, expert and novice, professional and amateur. The expert possesses knowledge the novice lacks; the expert is thus granted an inaccessible authority beyond reach of the layman.

As computer scientist Joseph \citeauthor{Weizenbaum76} observed in his 1976 study on the psychology of programming, knowledge of programming is overwhelmingly framed as a form of mastery, where it commonly induces a feeling of pleasure, even megalomania, in its subjects: "The extreme phenomenon of the compulsive programmer teaches us that computers have the power to sustain megalomaniac fantasies. But that power of the computer is merely an extreme version of a power that is inherent in all self-validating systems of thought" \autocite[130]{Weizenbaum76}. Expert programmers have variously repeated this refrain of psychological empowerment within their practices over the years:

\begin{quoting}The reward [for becoming a better programmer] is a more active involvement with a job you love, a feeling of mastery over an increasing range of subjects, and pleasure in a feeling of continuous improvement. \autocite[12]{Hunt1999}
 \end{quoting}

\begin{quoting}
Feelings of power and a narcissistic fascination with the image of oneself reflected back from the machine are common. \autocite[13]{Kay1977}
\end{quoting}
Although programming knowledge is typically framed in terms of mastery in this way, this is not a necessary or natural condition of procedural literacy but an ideological and historical one. By describing conventional practices of programming-oriented procedural literacy as forms of knowledge demanding mastery of a fixed medium, I wish to distinguish that model from alternative, less-developed constructions of programming in terms of interaction, mutual understanding and language. Observing children learning Logo programming in the 1980s, Turkle offered a helpful distinction between "hard" and "soft" mastery along these lines:
\blockcquote[qtd.~in][101]{Edwards1990}{
  Hard mastery is the imposition of will over the machine through the implementation of a plan. A program is the instrument of premeditated control. Getting the program to work is more like getting "to say one's piece" than allowing ideas to emerge in the give-and-take of conversation.…|T|he goal is always getting the program to realize the plan. Soft mastery is more interactive,…the mastery of the artist: try this, wait for a response, try something else, let the overall shape emerge from an interaction with the medium. It is more like a conversation than a monologue.
}
Edwards interprets Turkle's distinction from a more explicitly ideological perspective, noting that the division itself is false but nonetheless "plays a major ideological role" in a set of mutually-oriented relations between male-dominated computer culture, postwar militarized masculinity, and cultures of formal game-playing simulations or "microworlds": "In the microworld, as in children's make-believe, the power of the programmer is absolute.…For men, to whom power is an icon of identity and an index of success, a microworld can become a challenging arena for an adult quest for power and control" \autocite[109--10]{Edwards1990}. In contrast to a figure of hard mastery linked to Western rationality, Edwards associates the figure of "soft" mastery with anti-authoritarian hacker subcultures, "an ongoing, intersubjective process" of communication, and "shifting, contextually specific, emotionally complex relationships" \autocite[107]{Edwards1990}. It is this latter figure of "soft" mastery that I believe begins to approach a model of code literacy with the potential for collective political and ethical agency. In the next section, I will develop a theoretical foundation for such a model of critical code literacy by drawing comparisons between computational and legal procedure within a reading of Habermas's concept of the public sphere.

\section{Critical Code Literacy}
\subsection*{Code As Law}
Following Deleuze and Guattari's theorization of the concept of the "axiomatic" as a self-validating system of thought, Wendy \citeauthor{Chun2011} asserts that "software is axiomatic" to the extent that it "depends on the disciplining of hardware and programmers, and the desire for a programmable axiomatic code" \autocite[49]{Chun2011}. In its double-sided role as a system of writing and a pure abstraction of computational action that Galloway claims "is the first language that actually does what it \emph{says}" \autocite[qtd.~in][22]{Chun2011}, software idealized in this way conflates executable with execution, program with process, order with action. Code is thus the contemporary version of the mystical Word made flesh, the modern-day form of \emph{logos}:
\blockcquote[22]{Chun2011}{
  By doing what it >says,< code is surprisingly logos. Like the King's speech in Plato's \emph{Phaedrus}, it does not pronounce knowledge or demonstrate it---it transparently pronounces itself. The hidden signified signified---meaning---shines through and transforms itself into action. Like Faust's translation of logos as >deed,< code is action, so that "in the beginning was the Word, and the Word was with God, and the Word was God."
}
This figure of "code" as an inert, ideal intersection of abstract thought and mechanical calculation, widely adopted within the digital humanities and software industry alike, grounds popular cultures of procedural literacy in concepts of knowledge based around mastery and control of a fixed and orderly set of logical notations and mechanical operations. As Chun argues, this popular idea of software as axiomatic, as a logical self-validating system of thought transparently linked to computational action, is rooted in a desire for empowerment and control over an "internally consistent if externally incomplete microworld" \autocite[46]{Chun2011}.

In this section, I explore the theoretical construction of an alternate figure of procedural literacy, one opposed to this figure of software as an axiomatic, mastery-oriented form of literacy. Instead, I will develop the ethical-political implications of a figure of software conceived in terms of bureaucratic procedure, linked to communicative ideals of democratic deliberation  as expressed in Habermas's interpretation of modern legal theory. This understanding of procedure as judicial process, in its complex and nuanced deliberations of competing interests and interpretations, contrasts against the relative simplicity of conventional understandings of performative utterances as simply "doing what they say." As Chun points out, citing Judith Butler's argument in \emph{Excitable Speech}, this ideal model reflects a nostalgic desire for a simpler mapping of power, a mapping more akin to sovereign rule than structures of governmentality:
\blockcquote[28]{Chun2011}{
  Austinian understandings of performative utterances as simply doing what they say posit the speaker as "the judge or some other representative of the law." It resuscitates fantasies of sovereign---that is \emph{executive} (hence executable)---structures of power: it is "a wish to return to a simpler and more reassuring map of power, one in which the assumption of sovereignty remains secure." This wish for a simpler map of power,…is central to computers as machines that enable users/programmers to navigate neoliberal complexity. 
}
In contrast to this simple map of power, "code as law---as a judicial process---is, in other words, far more complex than code as logos" \autocite[28]{Chun2011}. For this reason, I find the articulation between code and law, between computational and legal "codes" and "procedures" to provide a good point of departure for reconstructing procedural literacy from the perspective informed by critical theory. One area the application of democratic principles to technical systems is within the contested interface between legal and computer code. There is a strong general pressure within the software industry towards a state of legal deregulation, including among free-software movements and software corporations that increasingly strategically draw upon open-source distribution models. The vast majority of competitive fields of software development are either self-regulated through industry-appointed standards bodies, or left to fend for themselves in a competitive, rapidly-evolving marketplace.

In line with this current of thought, a group of digital law scholars have embraced a concept originally introduced by Joel \citeauthor{Reidenberg1998} as \citetitle{Reidenberg1998}, a theory of the complex articulation between the democratic system of legal regulation and the more mercantilist systems of regulation through computer technology, now more widely known under the banner of Lawrence \citeauthor{Lessig99}'s popular slogan "code is law." While the concept of computer code has its origins in the idea of a legal code, legal and computer systems both have recourse to a common concept of "effective procedure." While regulating desired social behavior is often ambivalently possible through recourse to either form of regulation, the technical and professional discourse among the two is so strongly differentiated that conflating the two systems entirely would be too reductive. Under Lex Informatica, the accepted position is that in areas of digital law which deal directly with the public regulation of computer systems, any effective regulatory intervention requires a nuanced, pragmatic understanding of the combined effects of both computational and legal procedure, and requires a careful consideration of the tradeoffs in establishing and maintaining effective regulation through legal and/or computer systems.

The greatest technological edifice of our contemporary global public imaginary, the Internet and its loosely-coupled collection of web technologies, today hangs precariously in a balance of competing and constantly fluctuating corporate and government interests. The modern constitution of the public Internet is a system of codes governed not by an ideal democratic process drawing upon the informed consensus of a public conceived as collective, self-regulating authors, but by a complex, many-layered, ad-hoc aggregation of power and protocol optimized for global efficiency, written and rewritten by the fluctuating market forces of capital to capture and compete for market shares of largely powerless subjects construed as end users and technology consumers. The same may be said about the diaspora of specific programming languages, technologies and protocols that comprise the raw material of contemporary technological literacies.

The doctrine of Lex Informatica describes (and helps construct and legitimate) a porous interface between law and technology. Under this model, legal regulation of technical infrastructure is not the only avenue for intervening in technological futures against the current of market forces. If computer code indeed regulates its own production, public calls for code literacy can push beyond advocating mere mastery of public computing interfaces as end user products, and share in the same calls for assuming collective authorship and responsibility for civic justice that educators have linked to mass national language literacy as a constitutive component of participatory democracy since Dewey. In other words, a \emph{critical code literacy}\footnote{
  This term was briefly suggested by David \citeauthor{Berry2008}: "By highlighting the communicative dimension of social development, the need for technical education (a kind of critical code literacy), and the importance of the human at the centre of these struggles, [free/\emph{libre} open source software discourses] contribute to a humanistic turn in engineering philosophy" \autocite[192]{Berry2008}.
}
can advocate not only through legal-procedural democratic norms, but it can also effect change through public critique, research and development of computing systems with the aim of producing alternative procedural frameworks.

My argument through the rest of this chapter proceeds as follows: First, through a reading Habermas's critical theory of the public sphere in the context of its historical account of the development of mass literacy through print media, I argue that the rhetoric of literacy is linked to the idealization of a bourgeois public sphere. Next, I compare several unique qualities of Habermas's later theory of a proceduralist paradigm of law to similar considerations in the domain of software development. After considering several critiques and proposed expansions of the concept of the public sphere that consider technical mediation and pedagogy as influencing factors, I juxtapose the public sphere as a normative, regulative ideal against theories of critical-oppositional public spheres alongside Deleuze and Guattari's theme of minor literature. From this comparison, I propose a balanced perspective that recognizes the regulative, stabilizing function of the bourgeois public sphere but focuses on the development of minor, marginal and localized forms and institutions of critical discourse. Finally, I will compare several transformative programming environment projects in order to apply this critical framework to concrete software studies, and to reveal models and tactics of potential emancipatory transformation that could be mobilized toward contemporary critical code literacy practices.

\subsection*{The Bourgeois Public Sphere}
Habermas's influential project of Frankfurt school-informed critical social theory traces a history of the structural relation between the constitution and ongoing transformation of what he terms the "bourgeois public sphere" as a critical component of the new civil society, and the forms of self-reflexive governance and liberal democracy developed in modern European nation-states. In \citetitle{Habermas1989}, Habermas argues that the transformation of the institutions of print media in 18th century Europe from closed mechanisms largely controlled by nation-states to spaces of critical discourse marked an unprecedented transformation in the bourgeois public sphere, introducing a new collective political force of private individuals capable of granting or revoking a power of public legitimation to the prevailing authority's legal codes. Habermas idealizes the 18th-century public sphere as "a forum in which the private people, come together to form a public, readied themselves to compel public authority to legitimate itself before public opinion" \autocite*[25--6]{Habermas1989}.

The bourgeois public sphere upon which this legitimation depended was distinct from earlier forms of public presentation such as the classical Greek agora or Roman forum. Fueled in part by the development of technologies of print publication, this model of publicity was secured through an upper-middle class empowered by a newly public form of mass literacy, capable for the first time of sustaining collective, unofficial political debate beyond control of the public authorities. Habermas notes that the use of public deliberation as a medium of political resistance was inspired by and derived from the development of intersubjectivity through literary public spheres constituted by new mass genres of published novels and literary journals:

\blockcquote[54--5]{Habermas1989}{
  A political consciousness developed in the public sphere of civil society which, in opposition to absolute sovereignty, articulated the concept of and demand for general and abstract laws and which ultimately came to assert itself (i.e., public opinion) as the only legitimate source of this law.…The criteria of generality and abstractness characterizing legal norms had to have a peculiar obviousness for privatized individuals who, by communicating with each other in the public sphere of the world of letters, confirmed each other's subjectivity as it emerged from their spheres of intimacy.…

  The self-interpretation of the public in the political realm, as reflected in the crucial category of the legal norm, was the accomplishment of a consciousness fundamentally adapted to the institutions of the public sphere in the world of letters. In general, the two forms of public sphere blended with each other in a peculiar fashion. In both, there formed a public consisting of private persons whose autonomy based on ownership of private property wanted to see itself represented as such in the sphere of the bourgeois family and actualized inside the person as love, freedom, and cultivation---in a word, as humanity.
}
This eventual "blending" of the political and literary forms of public sphere constituted an ideal that became the basis for universal appeals to reason, freedom and humanity that grounded legal norms characteristic of Enlightenment discourse. The public use of reason that served as the medium of modern, participatory democracy was established by an overlapping set of diverse social formations including \emph{tischgesellschaften} (table societies), salons and coffee houses, all of which "had a number of institutional criteria in common":

\blockcquote[36--7]{Habermas1989}{
  First, they preserved a kind of social intercourse that, far from presupposing the equality of status, disregarded status altogether. The tendency replaced the celebration of rank with a tact befitting equals.…

  Secondly, discussion within such a public presupposed the problematization of areas that until then had not been questioned.… The private people for whom the cultural product became available as a commodity profaned it inasmuch as they had to determine its meaning on their own (by way of rational communication with one another), verbalize it, and thus state explicitly what precisely in its implicitness for so long could assert its authority.…

  Thirdly, the same process that converted culture into a commodity (and in this fashion constituted it as a culture that could become an object of discussion to begin with) established the public as in principle inclusive.…Wherever the public established itself institutionally as a stable group of discussants, it did not equate itself with the public but at most claimed to act as its mouthpiece, in its name, perhaps even as its educator---the new form of bourgeois representation.
}
Habermas traces the emergence of an idealized bourgeois form of public political discourse to an institutional transformation by which the rank-celebration, topic selection, and exclusivity typical of previous forms of official publicity and political influence were replaced by this "new form of bourgeois representation" based on forms of "rational communication" that were "in principle inclusive." An oft-overlooked aspect of Habermas's thesis evident in this passage is the strictly "bourgeois" nature of his conception of the public sphere: there is a structural relation between the interest of the wealthy middle class in the demarcation and preservation of their private autonomy (in terms of both property ownership and family values), and the promotion of a public sphere that would protect and preserve this autonomy. The bourgeois public sphere is effectively an idealized projection of the interests of the property-owning, family-oriented private sphere. The capitalist transformation of culture into commodity that established the private bourgeois liberal subject thus became the very means by which the medium of critical public literacy could be conceived, as a free, secular, all-inclusive marketplace of ideas.

As a critical model, this bourgeois public sphere was rife with its own ideological contradictions, as Habermas recognized and other critics of the concept have often remarked: despite its idealization as a space of universal reason representing the common interest of humanity, in practice only literate, educated, property-owning individuals could actually participate in such public spheres of understanding, to the extent that the public sphere was in practice defined by such practical exclusions despite its rhetorical appeal to inclusivity. The concept was indeed an Enlightenment ideal, normatively abstracted from real situations of political struggle that those groups and classes excluded or marginalized from the bourgeois public sphere faced. Nonetheless, Habermas claimed that the form of self-understanding that emerged from this idealism provided a concrete legitimating function for an emerging rule of procedural law, one that morally grounded its sovereignty on a reasoning public rather than on the guarded, secret authority of public nobility: "Just as secrecy was supposed to serve the maintenance of sovereignty based on \emph{voluntas} [will], so publicity was supposed to serve the promotion of legislation based on \emph{ratio} [reason]" \autocite[53]{Habermas1989}.

Although Habermas originally narrated the decline of this form of the public sphere in the face of corporate capture of the media and the creation of the culture industries (a thesis very much in line with the Frankfurt School tradition), his later research took a distinct turn away from the aesthetic fatalism derived from Adorno and Horkheimer's \emph{Dialectic of Enlightenment} toward a more progressive, liberal-democratic pragmatism. Thirty years after his analysis of the bourgeois public sphere, Habermas's \citetitle{Habermas-bfn} further develops the relation between the internal structure of positive law and the process of self-legitimation of a legal system by its subjects. In this work, Habermas's theoretical methodology shifts from historical materialism to a theory of "radical democracy," a more positive social-scientific attempt to portray an integrated society grounded in Enlightenment ideals of rational discourse yet nuanced enough to account for the complexity of modern social systems. Abstracting and removing the bourgeois public sphere from its historical, media-specific context, Habermas defines a "principle of democracy" that makes legitimate law possible in ideal, discourse-theoretic terms:
\blockcquote[110]{Habermas-bfn}{
  The principle of democracy should establish a procedure of legitimate lawmaking. Specifically, the democratic principle states that only those statutes may claim legitimacy that can meet with the assent of all citizens in a discursive process of legislation that in turn has been legally constituted. In other words, this principle explains the performative meaning of the practice of self-determination on the part of legal consociates who recognize one another as free and equal members of an association they have joined voluntarily.
}
This principle is an attempt to reconstruct the Enlightenment project of political self-determination through the abstract theoretical lens of communicative reason. As a normative ideal, it is rather a philosophical model against which institutional arrangements can be compared. The result is a discourse ethics aimed not merely towards individual competency and mastery of a given system fixed by univocal political or market power, but towards producing forms of discourse (and norms of discourse-production) that derive their legitimacy from a democratic "self-determination."
However, Habermas recognizes that such an idealized, abstract principle is not found in actually existing democratic associations. Such principles are "just as unavoidable as legal constructions as they are inappropriate as models for society in toto," and are "too concrete for social theory" \autocite[80]{Habermas-bfn}. On the other hand, he finds that modern sociology of law such as Niklas Luhmann's systems theory, which he says "conceives law only from the functionalist viewpoint of stabilizing behavioral expectations" \autocite[48]{Habermas-bfn}, is "renouncing any connection with the normative contents of practical reason" \autocite[2]{Habermas-bfn}, and is incapable of "restoring the explanatory power practical reason once possessed in the context of ethics and politics, modern natural law and moral theory, philosophy of history and social theory" \autocite[2]{Habermas-bfn}. In an attempt to reconcile these "two camps that hardly have anything more to say to one another" \autocite[6]{Habermas-bfn}, Habermas's work proposes a legal theory that mediates between the sociology of law and the philosophy of justice, or between "facts and norms," based on a theory of communicative action that attempts to retain the normative orientation of classical practical reason but without its aesthetic totalitarianism: such a theory "no longer resides in universal human rights, or in the ethical substance of a specific community" \autocite[296]{Habermas-bfn}, but instead "corresponds to the image of a decentered society" \autocite[301]{Habermas-bfn}.

\subsection*{Procedural Paradigm of Code}
Although the particular focus of Habermas's later work is on reconstructing the rational self-understanding of the legal system, the digital legal theory of Lex Informatica that emphasizes the structural correspondence between computer and legal code justifies extending such a research program toward technological environments becoming transformed into public code literacies. Next, I summarize several key themes from Habermas's reconstruction of a "procedural paradigm of law," and attempt to translate them into a more general theory of public procedure applicable to critical code literacy:

\emph{Accounting for the system complexity of modern society}. One of the most pervasive themes in Habermas's later work is that modern society is distinctly characterized by a degree of immense system complexity that constantly threatens to overburden our social systems. Taking the systems theory of Luhmann and Parsons as his point of departure, Habermas argues that it was under these pressures of systemic complexity that our society's fundamental form of organization underwent a transition from hierarchical stratification to functional differentiation, separating political, economic and administrative systems into highly specialized institutions disconnected from everyday human experience and restricted to self-contained modes of communication and knowledge representation. However, in contrast to Luhmann's politically-neutral sociological embrace of these formally-enclosed, autopoietic functional subsystems of society, Habermas maintains that despite modernity's pressures of complexity, its functional subsystems must still be coordinated through a common social environment of communicative action, or else embrace "the brash denial of reason altogether" \autocite[3]{Habermas-bfn}. Adapting terms from Husserl's phenomenology, he thus "distinguishes a >lifeworld< bound to the medium of ordinary language from >systems< steered through special codes" \autocite[55]{Habermas-bfn}, with the medium of law functioning "as a hinge between system and lifeworld" \autocite[56]{Habermas-bfn}.

This argument grounds the claim that an ethics of complexity is essential to a critical code literacy. Rather than an orientation that rewards practitioners with an increasingly-specialized mastery of self-contained codes and symbols cut off from everyday experience, an ethics of complexity strives for the reduction of unnecessary complexity in public code for the purpose of maintaining the "hinge" between the lifeworld of intersubjective communication and expert-oriented, complex functional systems that speak their own highly-specialized languages. Cultivated mastery of esoteric, specialized codes steeped in enigmatic abbreviations or private metaphors should be supplemented by a greater possibility of broad public understanding of technical concepts and structures, illustrated through collectively-authored vocabularies and languages that draw upon a common cultural background shared by as many potential practitioners as possible. Snow's remarks on computing cited earlier mirror Habermas's equally pervasive distrust of the tendency for expert discourse to become self-enclosed and autonomous in relation to the public it claims to represent, a situation which Habermas argues leads to "illegitimate power." An ethics of complexity thus points toward a critique of the cognitive burden of complex codes on a people's capacity for understanding, as well as the forms of expert knowledge and professionalism such complexity ultimately authorizes and makes necessary.

\emph{Establishing a postmetaphysical discourse ethics that structurally links, but does not reduce or conflate, law and morality}. With his endorsement of a proceduralist paradigm of law, Habermas argues for a balance between a purely formalist legal system that eschews moral claims entirely, and a system that derives its power directly from a single moral authority such as a dominant system of religious or community values: "Even if moral considerations are not selective enough for the legitimation of legal programs, politics and law are still supposed to be compatible with morality---on a common postmetaphysical basis of justification" \autocite[453]{Habermas-bfn}. Here, "postmetaphysical" is a term Habermas uses to mark his distance from metaphysics in general (and modern, post-Kantian philosophies of subjectivity and consciousness such as those of Hegel and Marx in particular) as well as to signal his alignment with American philosophers such as Rawls, Rorty and Dworkin. As a postmetaphysical philosophy can no longer assume the role of providing "criteria of validity," all that remains for it is to "mediate interpretatively between expert knowledge and an everyday practice in need of orientation" \autocite[17]{Habermas1992}.\footnote{ See Habermas's essays in \citetitle{Habermas1992} for further development of this position.}

In this pragmatic, mediating model, autonomous public spheres become the primary source of moral communication, where everyday practices of concrete forms of life are able to influence the institutionalized processes of otherwise autonomous, expert systems. In this capacity, Habermas has described the public sphere in mechanistic terms as a "a far-flung network of sensors that react to the pressure of society-wide problems and stimulate influential opinions" \autocite[300]{Habermas-bfn}, a "sounding board" and a "warning system with sensors" \autocite[359]{Habermas-bfn}. As a central lifeworld mechanism charged with mediating between expert knowledge and everyday practice, the public sphere idealizes a model of deliberation that, as Habermas emphasizes, is particularly attentive to individual life experience: "Systemic deficiencies are experienced in the context of individual life histories; such burdens accumulate in the lifeworld.…Problems voiced in the public sphere first become visible when they are mirrored in personal life experiences" \autocite[365]{Habermas-bfn}. Although this model of public spheres is not entirely reducible to such personal experiences of "systemic deficiencies" and injustice, it appeals to them in the first instance.

Along these lines, I agree that a morality of code must similarly establish a link between code and morality that is neither collapsed into a reductive technological autonomy nor dissolved entirely into a naive technophobia. Instead of either passively accepting our inherited technical system as morally determined, or opportunistically declaring that code is value-free without any relation to the moral norms of specific groups, we should strive to establish processes of technical governance and standardization that remain open to moral arguments that draw from personal, human experiences of injustice, inequality, or other such "systemic deficiencies" that affect the experience of everyday lives.

Here, I have in mind a perspective on technical discourses of "user experience" that have informed the design of technology since the mid-1980s, particularly as represented in the 1986 collection of essays titled \citetitle{Norman86} edited by Don Norman, and his later influence on Apple Computer's interface design practice. This framework of user-centered system design, designed "User Experience" at Apple in the mid-90s \autocite[see]{Norman95}, is a design methodology that incorporates information about the actual (or anticipated) experience of a product's users into future design iterations, the end result being a product that better serves the end user's needs. Traditionally, this field has focused on cognitive and psychological human factors related to the use of complex systems, with reducing deficiencies in usability as a primary moral concern. Along these lines, Latour argues that a "missing mass of morality" can found in the design of non-human technical artifacts, which impose their morality upon humans through material prescription of behavior, the "moral and ethical dimension of mechanisms" \autocite[157]{Latour92}. He illustrates this dimension through a hypothetical discussion of a door closing mechanism:
\blockcquote[158--9]{Latour92}{
  To be sure, the hydraulic door closer does not bang the noses of those unaware of local conditions, so its prescriptions may be said to be less restrictive, but it still leaves aside segments of human populations: neither my little nephews nor my grandmother could get in unaided because our groom needed the force of an able-bodied person to accumulate enough energy to close the door later. To use Langdon Winner's classic motto (1980): Because of their prescriptions, these doors discriminate against very little and very old persons.
}
As Latour hints at in this discussion of the morality of mechanisms, as the design of software interfaces address themselves to increasingly mass publics, such moral discourses of "user experience" should expand accordingly to accommodate increasingly complex discourses and negotiations among diverse populations. Such discourses must shift their moral perspective from an individual, cognitive emphasis on "the user" to address experiences among multiple and varied populations of users, categorized along relevant axes such as age, socioeconomic status, race, gender, and nationality, for example.

\emph{Distinguishing the efficiency or instrumentality of procedure from alternate forms of legitimacy}. Habermas's theoretical work maintains a consistent adherence to the critique of instrumental reason that originated in the social theory of the Frankfurt School. Habermas's distinction between system and lifeworld translates certain themes of a classical Marxist class analysis into a Parsonian structural-functional framework. He analyzes various forms of "legitimation crisis" within welfare state democracy, characterized in later work as lifeworld "pathologies," particularly the "colonization" of the lifeworld by system structures that pose a modern threat to social solidarity. The normative focus of Habermas's work is oriented toward a critique of the self-sufficiency of instrumental reason within modern democratic societies. In certain deliberative, democratic contexts, arguments and reasons provide the public legitimacy to decisions that purely systemic media such as money and administrative power cannot provide. In this context, \citeauthor{Lyotard1984-kb}'s recognition of "paralogy" as the proper form of legitimation within postmodern science \autocite[60--66]{Lyotard1984-kb} is the classic expression of opposition to the focus on univocal performance and efficiency found in twentieth-century positivist ideologies.

In relation to code literacy, this disjunction of efficiency and legitimacy can form the basis of a critique of efficiency as a response to a pervasive black-boxed, purely technical relation to code where operational efficiency is a primary determining value. Evaluating existing, highly-technical systems against more diffuse institutional values and moral norms is a good starting point for a substantial method of code critique that extends beyond goal-oriented performance metrics and Taylorist management practices all too common in the history of computing practice. Here, I have in mind the influence of twentieth-century managerial practice on the development and codifications of programming practice, and associated myths and ideologies that have become commonplace in computing culture that compromise the background knowledge of code literacy. One prominent example of this is the persistent myth of the "10x" or "superprogrammer" among software professionals, a claim that software researcher Steve McConnell demonstrates is supported by decades of productivity research suggesting that "there are order-of-magnitude differences among programmers" in terms of workplace productivity \autocite*{McConnell10}.\footnote{See also McConnell's \autocite*{OriginOf10x}.}

\subsection*{Expanding the public sphere}
I find in Habermas's discourse theory a set of criteria by which we can critically estimate the legitimation of procedural norms in relation to a public use of reason, but this is where a strict reading of Habermasian ideals of the public sphere reaches its limitations.

\emph{First}, as I have addressed by expanding these communicative principles to the domain of computer programming languages, I believe that the legitimation criteria Habermas outlines shouldn't be limited to official, institutionalized legal procedure. If recourse to democratic principles is to be upheld in complex procedural environments regulated both by legal institutions and computer systems, and if claims for the potential constitution of a code-literate public are to have any lasting significance, code literacy advocates must make the case that such a public can and must assume responsibility for the self-regulation of its technological systems according to similar democratic principles.

\emph{Second}, although Habermas's work is a powerful affirmation of the legitimation of norms through the self-constitution of legal systems, he fails to include the possibility of the critique of technology design in his theory; rather, according to several accounts, Habermas actively resisted incorporating the public spread of computer-mediated communication into his later work by attributing essential ideal communicative qualities to a physically-embodied >face-to-face< encounter and the living, natural language of the lifeworld that the legal system draws upon and that can't be replicated in other institutional forms. Responding to Habermas's claim that "The publics produced by the Internet,…remain closed off from one another like global villages" \autocite[qtd.~in][116]{Poster01}, Poster argues that Habermas "clearly has got things wrong" \autocite[116]{Poster01}, because electronic media systematically deny the sorts of political organization that Habermas's theory models:
\blockcquote[181--2]{Poster01}{
  For Habermas, the public sphere is a homogeneous space of embodied subjects in symmetrical relations, pursuing consensus through the critique of arguments and the presentation of validity claims. This model, I contend, is systematically denied in the arenas of electronic politics. We are advised then to abandon Habermas's concept of the public sphere in assessing the Internet as a political domain.\footnote{
    Brun similarly laments that Habermas's recent work demonstrates an "obvious aversion to accepting the Internet as part of the public sphere," citing a paper in which Habermas dismissed the democratic potential of computer-mediated communication over the Internet as "the rise of millions of fragmented chat rooms across the world" that merely produces "isolated issue publics" \mancite\autocite[qtd.~in][]{Brun07}. \citeauthor{Rheingold07} shares a similar impression that Habermas "simply does not understand the Internet."
  }
}

Despite any productive engagement with computer-mediated sociopolitical organization in his later work, abandoning Habermas's concept of the public sphere entirely due to recent technological changes in communication media would be throwing the baby out with the bathwater. Instead, I find Kellner's critical analysis of Habermas's work on the public sphere to be a productive middle ground of generative critique. Though Kellner argues that "Habermas' project is undermined by too rigid categorical distinctions between classical liberal and contemporary public spheres, between system and lifeworld, and production and interaction" \autocite*[281--2]{Kellner00}, he nonetheless applauds Habermas for having produced a project which "has generated a wealth of theoretical discussions and has provided normative bases for social critique and democratization" \autocite[271]{Kellner00} For these latter reasons, Kellner proposes to expand Habermas's concept of the public sphere in recognition of its contributions to participatory democratic theory: "An expanded public sphere and new challenges and threats to democracy render Habermas' work an indispensable component of a new critical theory that must, however, go beyond his positions in crucial ways" \autocite[282]{Kellner00}.

Kellner's "critical intervention" links the public sphere to a critical media pedagogy through an emphasis on a more expansive notion of the multiple cultural and technological literacies necessary for democratic participation in multicultural society.\footnote{
  Kellner gestures toward this focus in his critique of Habermas: "[Habermas] omits the arguably necessary presuppositions for democratic deliberation and argumentation---an informed and intellectually competent citizenry. Here the focus should arguably be on education and the media, for schooling and the media play a key role in enabling individuals to be informed, taught to seek information, and, if effectively educated, to critically assess and appraise information, to transform information into knowledge and understanding, and thus to make citizens capable of participating in democratic discussion and deliberation" \autocite[277]{Kellner00}.
}
While attending more closely to the significant state and corporate interests that have shaped the technological infrastructures of television and radio broadcasting systems, he exhorts the "critical-oppositional intellectual" to intervene within these technologically-mediated public spheres on behalf of social causes: "intellectuals in the present moment must master new technologies and,…there is thus a more intimate relationship between intellectuals and technology than in previous social configurations" \autocite[438]{Kellner95}. Such an argument leads to a concept of critical technological literacy that links individual technological empowerment to social change. However, Kellner falls back upon the passive trope of mastery and tool-use, writing that "computer literacy involves technical abilities concerning developing basic typing skills, using computer programs, accessing information, and using computer technologies for a variety of purposes ranging from verbal communication to artistic expression" \autocite[116]{Kellner98}. This does not adequately interrogate the formation and evolution of technologies themselves, and overlooks how the design of interfaces, codes and standards necessary for participation in public spheres based on new technologies can themselves produce or reinforce the sociocultural inequalities that egalitarian efforts to provide access and education will not alleviate.

Returning to my earlier example, such a model of computer literacy would go no further than the educational model of Codecademy, emphasizing "hard mastery" of the syntax and affordances of the dominant Internet communication vocabularies and platforms such as HTML, CSS and JavaScript, perhaps even higher-level web application products such as WordPress or Blogger, as methods of empowering upwardly-mobile communities to participate in social and political expression through conventional forms of computer-mediated communication. While undoubtedly progressive, this model of critical media literacy tends toward a passive, pragmatic embrace of useful technological skills, which are largely commodified and guided by corporate interests. Because the production and control of technical literacies, platforms and standards are increasingly important activities in contemporary social struggles over emerging forms of public digital communication, they should also be included within the scope of a critical literacy and subject to the normative scope of an ideal public sphere of democratic participation. Such an emphasis on technical production would be a starting point for a model of social research that would effectively link Kellner's inclusion of media literacy within an expanded public sphere to the technical forms of procedural literacy that Bogost, Mateas and others have espoused.

Another group of criticisms leveled against Habermas's bourgeois public sphere concept \autocites[e.g.,][]{Negt93}{Fraser90}{Warner92} have advanced a more fundamental critique that can't as easily be accommodated by a progressive expansion of the concept of the public sphere, but demands a dialectical reconsideration of the Enlightenment ideal of public reason itself. The general argument of these and other similarly critical positions is that marginalized groups remain excluded from the idea of a public sphere presumed to be universal and unified, and in the argument's strongest form, such idealized public spheres are fundamentally constituted by such exclusions. For example, \citeauthor{Fraser90} notes that in the Enlightenment-era periods from which Habermas derived his initial model of the bourgeois public sphere, women, racialized ethnicities and the plebeian classes were often formally excluded from official political participation as well as informally marginalized through dominant social-cultural protocols of style and decorum. \citeauthor{Warner92} critically reads the bourgeois public sphere as a "minoritizing logic of domination" \autocite[384]{Warner92}, and "a logic of abstraction that provides a privilege for unmarked identities: the male, the white, the middle class, the normal" \autocite[383]{Warner92}. From these perspectives, the public sphere as a singular ideal of universal, democratic participation is reimagined as a normalizing model of liberal-pragmatic hegemony, against which a plurality of radical, oppositional, minoritarian public spheres contend.

\emph{Third}, as a thematic summary of the above critical responses to Habermas's communication-theoretical construction of the public sphere, I take Deleuze and Guattari's multivalent figure of minor literature as a representative image. In his study of Deleuze and Guattari's concept of "minor" politics, \citeauthor{Thoburn2003} writes that "Whilst the minoritarian is concerned with expression,…such expression is not >communication< in the sense of the manifestation of an identity or a process of bringing people into a public sphere where all may be heard. The question is rather one of the invention or creation that occurs in a cramped space" \autocite[20]{Thoburn2003}.

Taking this critique into account, I propose a hybrid approach that acknowledges the pervasive and necessary component of the functional differentiation of society that produces specialized systems and forms of expert knowledge, but also necessitates a critical component that resists the passive, technocratic totalization and domination of those forms. Critical code literacy advocates and cultivates not just technical mastery but informed technical critique. What will such a critique look like in practice, applied to the kinds of code we find at work in computer systems? In the following section, I will transition from the above discussion of the theoretical foundations of a procedural literacy grounded in ideal public spheres to develop a more concrete practice of critical software study emerging from these principles. To this end, I will look beyond the merely cognitive dimensions of programming notations and the technical efficiency mastery of a given technical code may offer an individual user, in order to form an ethical-political critique of a procedural literacy whose legitimacy derives from its status as language, governed by the mass public it addresses.

\section{Critique of Programming Language Systems}
I have developed a concept of critical code literacy derived from Habermas's ideal of democracy linked to a public sphere constituted by literate civil society, extended with several modifications. This concept of public literacy extends beyond a liberal-individualist, cognitive or skill-oriented notion of mastery to incorporate an ethical responsibility towards a critical self-reflection on the relation to the public sphere toward which a particular code's adoption is addressed.

In this section, I shift from theory to a more practical exploration of potential avenues for critical action within the specific domain of contemporary programming language systems. To accomplish this, I take guidance from Feenberg's concepts of the ambivalence of technology and alternate modernities, which synthesize a reading of Habermas, Marcuse and Foucault into a critical theory geared toward a critique of technological rationality. Agreeing with Kellner that Habermas "ends up pessimistically decrying the rising tide of technocracy without providing a persuasive alternative" \autocite*[85]{Feenberg1993}, Feenberg's work offers a critical theory of technology embedded in political praxis:
\blockquote{
  All modern industrial societies stand today at the crossroads, facing two different directions of technical development. They can either intensify the exploitation of human beings and nature, or they can take a new path in which the integrative tendencies of technology support emancipatory applications. The choice is essentially political. The first path yields a formally biased system that consistently reinforces elite power. The second path requires a concretizing application of technical principles, taking into account the many larger contexts on which technology has impacts. These contexts reflect potentialities---values---that can be realized only through a new organization of society. \autocite*[188]{Feenberg2002}
}
This new organization of society that Feenberg imagines is not a preconceived utopian totality, but merely the possibility of an alternative form of modernity based on indeterminate social values and human practices not reduced to a technical relation of efficiency: "Nature as a context of development is,…a dialectical limitation that invites transcendence through adaptation.…Adaptation maintains the formal character of the modern concept of freedom and therefore does not reduce individuals to mere functions of society. Freedom lies in this lack of determinacy" \autocite[190]{Feenberg2002}.

Feenberg argues that the democratization of technology requires the subject to not only contextualize existing technology design as socially and historically constructed, but also to consciously and productively inherit, adapt and reconstruct such existing designs according to concrete values and principles. I find that this argument extends Habermas's general theory of ethical-political discourse, as "affirmation of a form of life in light of critically appropriated traditions" \autocite*[163]{Habermas-bfn}, into the domain of technological artifacts. Taking inspiration from Feenberg's philosophy of technology, I will examine how concretizing, emancipatory applications of technology emerge in the context of contemporary procedural literacy through a critical comparative analysis of several programming language systems that aim to transform procedural literacy through technological design.

\subsection*{From cognitive usability to ethical-political counter-hegemony}
Before this case-study comparison, I wish to further clarify how my critical approach to public software contributes to an ethical-political discourse of software, as distinguished against existing forms of discourse within the field of software studies. To demonstrate this difference I rely on \citeauthor{Green1989}'s \citetitle{Green1989} framework, a well-known heuristic of system usability commonly applied to the analysis of programming languages and environments. In this heuristic, a "cognitive dimension" of a notation is a "characteristic of the way that information is structured or represented, one that is shared by many notations of different types and, by its interaction with the human cognitive architecture, has a strong influence on how people use the notation and affects whether the strategy of opportunistic planning can be pursued" \autocite[448]{Green1989}. This framework evaluates the cognitive implications of an idealized user's adoption of a specific notation. In his original paper, Green analyzes the SmallTalk programming language, noting how certain choices in the design of its notation encourage or impede specific types of language use according to several broad cognitive dimensions.

To develop a mode of critique of programming language systems, I propose a translation of Green's analytical framework from an assessment of a notation's cognitive dimensions into ethical-political ones as outlined in my previous section. One of Green's more celebrated cognitive dimensions, "viscosity," is defined as a notation's "resistance to local changes," which he uses to compare the effect a choice of notation has on the cognitive burden to make incremental, isolated changes. For example, changing the name of a single variable may require the programmer to also update all of the references to the object throughout an entire program, which can be a significant burden in programming environments where this change is not automated. By extension, I propose we assess a programming environment not just with regards to cognitive viscosity in terms of a notation's cognitive resistance to local changes within a given program but also with regards to its ethical-political viscosity, in terms of its resistance to broader changes to a language itself in response to ethical-political arguments that arise from within the public spheres of its projected audience.

For many software professionals, the various notations, idioms, and technical-cultural vocabularies and histories common to all mainstream programming languages have either become so naturalized as a basic, universal structure of rational-computational thought, or justified according to globalizing logics of efficiency, market dominance, technical standards and industry best practices, that it is difficult to imagine any ethical-political argument cutting across them. Why would the syntax, grammar, or vocabulary of a programming language have ethical or political implications, as long as its users can acquire the necessary literacy to meet their immediate needs? This is the question a critique of programming language systems should address.

I should also distinguish such a focus on ethical-political arguments from common forms of free/\emph{libre} software advocacy and software studies research. On the one hand, Richard Stallman's campaign for free software is a popular, influential, explicitly ethical response to the commodification of culture in the form of proprietary software; however, as David \citeauthor{Berry2008} has noted, the Free Software Foundation has failed to "widen its discourse from that of deontological ethics and community-shared processes for the production of social goods to that of a wider discourse of democracy" \autocite*[185]{Berry2008}, short-circuiting discussions of the political implications of the project's libertarian-leaning ethical standpoint, resulting in a sort of "union or guild-like structure for computer programmers" \autocite[101]{Berry2008}. Within software studies research I take Lev \citeauthor{Manovich13}'s \citetitle{Manovich13} to be a representative example which, although generally supportive of open source software, displays an instrumental attitude towards the production and consumption of software by focusing on popular software packages without addressing ethical-political legitimacy. Manovich chooses to discuss "Photoshop rather than Gimp, and Illustrator rather than Inkscape" simply because the former software products are more popular, because his interest is in "describing the common user experiences, and the features of media aesthetics common to millions of works created with the most common authoring tools that are all commercial products" \autocite[50--1]{Manovich13}.

One response to such normalizing approaches to technology studies, drawing parallels to critical accounts of "Global English," involves the assertion of cultural-linguistic diversity and minority as a form of resistance to monolingual technocratic colonization. Along these lines, I consider the following anecdote from Yuri \citeauthor{Takhteyev12}'s ethnography of Brazilian software development, where a couple of young professionals discussed the story of a failed Microsoft project to translate the company's Visual Basic scripting language to Portuguese:

\blockcquote[53]{Takhteyev12}{
  This must be one of the stupidest ideas ever! they exclaimed at the same time. How would you even do it? asked Fabio.…Portuguese just isn't a good language for programming languages. The grammar is too complex. What would you write in the end of the function? "Retorno"? "Retorne"? "Retornar"?…In English it all makes more sense, he concluded.
  
  Fabio's comments about the idea of using Portuguese keywords as keywords in a programming language do not merely acknowledge the de facto dominance of English in software, but also naturalize this dominance.
}
The naturalized dominance of English-language programming languages that made the idea of using keywords in any language other than English unthinkable to these young Brazilian programmers was maintained by the domination of local software industries by the products, platforms and technologies of English-speaking American corporations, compounded by general English-language hegemony in multinational business. The essentially English-language logic of software has been recognizably linked to global business and politics since at least 1984, when Steve Jobs commented to the French president Francois Mitterrand on a visit to Silicon Valley: "The problem with French software, Mr. President, is that it's written in French. You can't sell it" \autocite[qtd.~in][B4]{Dobbs1986}. In response, Mitterrand voiced his frustration with Silicon Valley's English-language technocultural power at a 1986 meeting of the French Academy, asking, "Must we translate into English the orders we give machines?" \autocite[qtd.~in][B4]{Dobbs1986}.

Joe Lockard's account of the arrival of "cyber-english" as a "superdominant english specifically intrinsic to a computer-mediated technology base" raises the possibility of "counter-hegemonic software development" \autocite{Lockard96}, a critical perspective that Rita \citeauthor{Raley2003} advances by "illustrating a link between the mechanized code of machine languages and an updated pasigraphy" \autocite[306]{Raley2003}: "With Global English as a precursor network and medium of late twentieth-century communication, computer languages maintain a parallel currency and legitimation" \autocite[307]{Raley2003}. Such arguments link the desire for universal language, whether in the form of Global English or the mass adoption of monolingual programming languages, to purely instrumental views of language as the communication of information, and of literacy as developing mastery of an external relation to dominant structures of knowledge and power.

\subsection*{Programming Languages as Social Practice}
I will next analyze and compare several programming languages from a perspective considering not just their abstract cognitive utility or set of features and functions, but also their formation and governance as concrete social practices in relation to specific populations and histories. I wish to shift from a notion of programming that affords abstract procedural power to a generalized user-subject to a concept of programming as a social practice, one that articulates global technocultural platforms to local, concrete social histories and vice versa. Through a comparison of JavaScript against the alternate projects of Processing and DrScheme, I suggest that the latter offer particular models of public programming that productively diverge from the adoption of languages organized through technical consensus.

\subsubsection*{JavaScript}
JavaScript's relatively simple syntax, error-tolerant grammar, and particularly its early adoption within a majority of web browsers cemented its place in the global Internet economy. Thanks to the commitment of all major web browser developers to interoperable, standards-based implementations of JavaScript (Microsoft Internet Explorer, Mozilla Firefox, Apple Safari, and Google Chrome), and their active participation in the language's ongoing standardization process and evolution within the ECMA organization, programs written in JavaScript are able to execute relatively uniformly across a large share of the modern web's computing platform infrastructure. As the programming platform with the largest market share of web browsers, JavaScript is viewed today as the Global English of the Internet, or the "programming language of the web" \autocite{Codecademy}.

JavaScript's long rise to prominence within the software industry can also be attributed to an effective strategy of constituting an expansive technical public across a dominant majority of the software industry through partnerships, marketing, licensing and standardization. Originally designed in 10 days as an internal extension for Netscape's Navigator 2.0 web browser, JavaScript was publicly announced in 1995 in partnership with Sun and marketed as "complementary to and integrated with" HTML and Java, Sun's then-dominant programming platform that could also run simple programs (called "applets") on web pages through its own browser extension \autocite{Netscape95}.\footnote{ See also \citetitle{Severance2012} \autocite{Severance2012}.} Over the next year, Netscape and Sun deployed a particular strategy of public technical governance, "work[ing] closely with ECMA, IETF, WC3 [sic] and others to advance Java and JavaScript as the standard development environments for Internet and Intranet applications," aiming for "the establishment of JavaScript as an open Internet standard" \autocite{Netscape96}.

JavaScript's reputation as an open standard became the focus of a public controversy surrounding Apple's decision to unilaterally ban Adobe's Flash platform from its popular iOS devices in April 2010, a controversy that tech journalists later linked to Flash's decline and JavaScript's subsequent rise to prominence as the central programming language component of the HTML5 platform.\footnote{
  See \autocite{Jobs2010}. Many tech pundits later attributed Jobs's public feud with Adobe for "killing Flash" \autocites[e.g.,][]{Manjoo10}{Isaac2011}{Lawler12}.
}
The Flash platform and its ActionScript programming language are also made available free of cost to developers, but its design is decided entirely by Adobe and is subject to more restrictive licensing agreements. Running a Flash program on a device requires users to install a runtime of proprietary compiled software, developed and maintained by Adobe, providing programs written in ActionScript/Flash high-level functions such as a graphical canvas and device input. Although developers may access documentation, API references and function libraries to author Flash programs, the underlying software actually implementing these functions on specific machines is privately held as a copyright and trade secret essential to Adobe's proprietary licensing model.

Despite JavaScript's reputation for openness, however, the software-industry consensus that JavaScript represents can also be viewed as a sort of liability and a form of resistance against socially-induced change. In contrast to the public legitimacy accorded to legal codes developed through equitable constitutional procedures (where, for example, the display of illegitimate influence such as political corruption or unethical lobbying could prompt public controversy with future political consequences), the specification and standardization of JavaScript through ECMA's institutional oligarchy of software industry professionals does not provide any substantial opportunity for critical public deliberation on behalf of the non-professional publics that mass code literacy campaigns such as Codecademy address. Without a more open incorporation of broader public interests and the possibility of public controversy to impact the procedural authority of the oligarchy itself, nothing in its constitutional structure would prevent stakeholders from accepting design aspects that may further private interests at the expense of the public. For example, design decisions making the language more complex or difficult for non-professionals to understand or adapt (for example, the unique behavior of JavaScript's undefined primitive or its prototype-based inheritance chain, or the addition of new functions in the language's standard vocabulary, or the verbosity of its standards documentation) may be less costly for ECMA's voting constituency of software professionals to develop the requisite expertise compared to the broader public affected by such design choices.

Although I find Geoff Cox's assertion that "JavaScript is proprietary, indeed owned by Google" \autocite*[83]{Cox2012} a bit hyperbolic, I do agree with the general sentiment expressed by \citeauthor{Berry2008} that "Web 2.0 companies, such as Google" can be understood in light of the "co-option of FLOSS [free/libre and open source software] into new models of production" \autocite[xiii]{Berry2008}, resulting in an industry-oriented standard not equally open to a broader mass public. To the extent that such a mass public is excluded from participating in the JavaScript language's industry-oriented evolution, efforts directed towards technical education in details of the language can be viewed as developing a technical-vocational mastery as opposed to advocating a more critically-reflective public form of literacy. A bureaucratically-designed language such as JavaScript allows non-professional practitioners to develop a functional literacy relatively quickly, but such literacy is so heavily determined by the oligarchical motivations of its industry designers that it actively shields the learner from the deliberations and decision-making processes that formed its environment in the first place, ones that might otherwise be contested within a differently-constituted public. Learners may indeed be empowered by JavaScript's functional ability to create web applications that can be executed on a majority of modern browser platforms across a wide range of software and hardware vendors, but the notation's political viscosity also nonetheless imposes a technical-linguistic dominance upon those minor languages and cultures less represented within its technical consensus.

\subsubsection*{Processing}
Processing is a special-purpose programming framework that I contrast against JavaScript to emphasize differences both in its constituted public as well as the governance of its language's development. Developed around 2000, Processing is technically not a full-fledged programming language of its own but rather an API (application program interface) layer on top of Sun's Java programming platform.\footnote{
  The Processing API was also later ported to the JavaScript platform as a sister project, Processing.js.
}
Despite inheriting Java's technical infrastructure and low-level syntax and grammar, the design of Processing incorporates several sustained critiques of the implicit context of mainstream, general-purpose, professional programming environments, including that of Java itself. Through its streamlined, simplistic interface and its promotion and cultivation of an open-source, do-it-yourself community of non-programmers, it exemplifies a radical reappropriation of an existing technology platform for alternate ends.

In his dissertation titled \citetitle{Fry2004}, Processing creator Ben \citeauthor{Fry2004} describes his project as "a tool for developing visually-oriented software. It was conceived of as a way to introduce programming concepts to designers, and design concepts to programmers" \autocite[123]{Fry2004}. In contrast to industry-standard programming languages such as Java or C++ used by professional programmers across all sectors of the software industry, the Processing environment was built to support a particularly narrow application domain, one directly suitable for the information visualization goals of the artists and designers within the Aesthetics and Computation Group (ACG) at the MIT Media Lab. Many of Processing's key features were an evolution of Design By Numbers, an earlier project headed by ACG director John Maeda, and Fry mentions that Processing "began as a >next generation< Design By numbers" \autocite[125]{Fry2004}. Both frameworks were used extensively in the ACG's own research and experiments.

The notable functional differences in Processing are evident in the environment's streamlined graphical user interface (GUI) and application programming interface (API). First, with its simple GUI, Processing subverted the text-based, command-line paradigm typical of most programming languages (e.g., type, compile, run, debug) by providing an interface containing parallel code-input and graphics-output windows, along with intuitive graphical play and stop icons to control program execution through a familiar analogy to consumer electronic devices. These features, along with free program distribution and a simple installation process, made it possible for visual designers to begin scripting simple visualizations without demanding knowledge of computers beyond familiarity with the Windows operating system. Second, the Processing API was intentionally designed with a minimal set of functions relevant to interactive graphics applications, "designed to be terse vocabulary that provide only the most relevant features used by the greatest majority of users" \autocite[129]{Fry2004}. Additionally, Processing hides complex Java concepts such as classes, packages and library imports behind scaffolding built into the program editor, so beginners are no longer confronted with confusing, abstract program semantics of required lines of boilerplate Java code such as >\texttt{public static void main(String[] args) \{\}}< before programming simple visual actions like "\texttt{line()}" to draw a line or "\texttt{rect()}" to draw a rectangle.

These various features, largely oriented toward abstracting the existing Java platform in the direction of accessibility, ease of use, and reduced complexity for the specific purpose of authoring specific genres of visual media, have made Processing a favorite among procedural literacy advocates \autocites[e.g.,][110]{Mateas05}[105]{Manovich13}. Although I also commend such technical innovations for instrumentally supporting the "quick development of media projects" \autocite[105]{Manovich13}, I wish to further emphasize the project's particular model of open governance structuring its evolving social practice in relation to a specifically-constituted community of (non-expert) practitioners. Along these lines, it can be argued that the regulative structure of the Java ecosystem from which Processing emerged had systematically excluded the interests or concerns of the class of end user programmers that constituted the Processing community, who could better participate in a separate programming paradigm. It is also worth noting that, in addition to being a free and open-source project, Fry institutionalized the ongoing governance of the language through the Processing Foundation, a nonprofit established in 2013 which notes that its largest source of funding comes from individuals donating less than \$100.

Lacking the aspiration to abstractly represent a unified, universal, public programming community, Processing exemplifies what Bonnie Nardi has called "end user programming," the design of a restricted programming environment within an application-specific domain. In this way, Processing is comparable to other restricted programming environments such as spreadsheet scripting languages. Such projects advance the question of "one or many programming languages?" in favor of the latter, demonstrating that something can be gained by adapting a programming environment toward specific application contexts. However, in contrast to Microsoft Excel, for example, I believe that the steps Processing has taken to establish a sustainable model of community-driven governance better exemplifies the construction of a public sphere established in a reciprocal relationship to its participants than other end user programming platforms demonstrate.

\subsubsection*{DrScheme}
My final case study concerns the relationship of public education standards to the constitution of procedural literacy for a mass public. While Processing emerged from and is designed for an existing community of new media practitioners and visual designers and can be understood considered within this bounded, application-specific context, a rising tide of contemporary procedural literacy advocacy reaches beyond specific end user programming contexts in attempts to teach universal principles of "computational thinking" \autocite{Wing06} not as a special skill but as a basic literacy, alongside elementary mathematics and English. DrScheme is one such early project that, while not as widely adopted as JavaScript or even Processing, presents a unique model of integrating procedural literacy within public education worth considering in the context of public sphere criticism.

The DrScheme programming language environment was constructed by \citeauthor{Felleisen04} for the "TeachScheme!" project, an educational initiative that "aims to move programming courses into the core of secondary school curricula" \autocite[57]{Felleisen04}. In their paper titled \citetitle{Felleisen04}, \citeauthor{Felleisen04} find fault with existing computer science curricula, such as introductory courses teaching Java and similar industry programming languages, for their "prevailing but outdated view of programming as a vocational activity. Secondary school educators and administrator [sic] simply don't understand the power of programming and its potential role in the core of a liberal arts curriculum" \autocite[56]{Felleisen04}. Specifically, by training in "vocational" programming languages and using complex, professional development environments for software composition, "investing energy into the study of complex grammars and programming environments distracts teachers and students from the true nature of programming" \autocite[56--7]{Felleisen04}. In order to improve upon current state of programming education, the authors developed DrScheme, a combined program development environment (PDE) and a hierarchical, curriculum-oriented series of programming languages designed to introduce programming concepts to beginners. Derived from the Scheme programming language (itself a minimalist variant of Lisp, as was Logo), this progressive series of language environments begins with a basic functional language that mirrors the syntax of basic algebra as closely as possible, growing to support more advanced functional language constructs such as assignment operators and eventually introducing the "notoriously difficult notion of state in programs" \autocite[60]{Felleisen04}, which is typically a more fundamental, introductory concept in mainstream programming languages.

The convincing argument the authors use to advocate using their curriculum to teach programming instead of more widely used, industry-standard languages such as C++ or Java is that the functional style and syntax of DrScheme enables its core concepts to be more directly grafted onto existing K-12 mathematics instruction standards (particularly algebra), suggesting that DrScheme's language constructs integrate more easily into the existing institutional codification of elementary education: "An animation is a mathematical function (from time to scenes); an interactive, graphical program is a mathematical expression; and a family of web pages is the result of some more mathematics" \autocite[129]{Felleisen10}.

\citeauthor{Felleisen10} describes later experimenting with designing a transition curriculum, intended to translate the basic concepts learned from the TeachScheme! program into real-world programming: "On the downstream side, students must see how the design principles in HtDP [How to Design Programs, a book describing their program design method] apply to class-based, object-oriented languages such as Java. These languages are what students need for their first co-op or internship" \autocite[130]{Felleisen10}. This suggests that even though the streamlined model of computing offered by the DrScheme programming environment has stronger affinities to the form of mathematical concepts encountered in the classroom, the TeachScheme! project still believes that a hybrid or transitional segment is still practically necessary to link these concepts to industry.

One of the most interesting aspects of the TeachScheme! curriculum is that it combines careful design changes to the syntax and grammar of a standard programming language environment with changes to the design of a computer science educational curriculum. However, I believe this emancipatory potential is largely stifled by the curriculum's strategy of simultaneously integrating with (and implicitly adopting the instrumental pedagogical attitudes of) existing K-12 math standards and industrial software practice. In this context, the specific details and nuances of the DrScheme language and its development environment emerge as a technological optimization of existing, externally-defined pedagogical goals rather than as a vision of a new form of code literacy constituted within a mass public. Its radical design critique of industrial programming languages through its use of scaffolding and functional programming constructs would be more convincing if also articulated with an expanded vision of computational thinking that extended beyond more engaging algebra instruction, or more efficient training for future Java engineers. Otherwise, DrScheme remains a toy programming language for young preprofessionals, a preparatory pedagogical tool in the service of existing coding paradigms, and constrained by the necessity to transition its students into the "real world" of global software development.

The tensions revealed within DrScheme and the TeachScheme! project reflect potential articulations and oppositions between the institutions of elementary public education, software industry standardization, and alternate, critical-oppositional or emancipatory concepts of literacy. Although TeachScheme! provides a productive model for aligning procedural literacy with public institutions of education, thus potentially relating to a mass public in a way that a programming environment built around industry consensus such as JavaScript could not achieve, it remains an open question whether such a project could be effectively linked to a more transformative literacy project that would not just enhance existing pedagogy standards but transform literacy practice in the process.

\section{Conclusions}
\epigram{Computers' use of symbols, like the use of symbols in language and mathematics, is sufficiently disconnected from the real world to enable them to create splendid nonsense.…It is just this realm of apparent nonsense that must be kept open for the developing minds of the future. Although the personal computer can be guided in any direction we choose, the real sin would be to make it act like a machine! \autocite[244]{Kay1977}}

The discussion within this chapter is still far from concluding the open debates and ongoing developments of procedural literacy; rather, my primary contribution has been to open up the relatively narrow debate to the broader political and philosophical themes of the public sphere, in order to expand the historical frames and conceptual horizons of research and development around the topic of procedural literacy as a central pedagogical facet of the ludocapitalist subject. We still largely lack an institutional understanding of public procedural literacy that resists notions of universal rationality aligned with the professional technical consensus of the software industry, and that instead support the production of alternate, local procedural literacies aligned with concrete social histories. The speculative work of this chapter makes such a perspective thinkable.

I have argued in this chapter that critical procedural literacy demands more than promoting a passing familiarity with popular programming frameworks, or familiarizing a mass public with expressing oneself through a range of expedient procedural authoring tools. Contemporary forms of mass literacy are structured in relation to the democratic public spheres they sustain, so a critical concept of literacy must grapple with the question of how its languages and media technologies maintain their representative legitimacy within the normative constraints of democratic procedural paradigms. While modern constitutional law has been studied under such a framework for quite some time, its extension to the production of public technical knowledge has been less well tested.

In contrast to comparing the harsh realities of computing platforms and programming  languages up against Habermasian ideals of publicity, I have offered several generative themes that could inform a critical orientation to procedural literacy, including Deleuze and Guattari's concept of minor literature and Feenberg's concept of the ambivalence of technology. Through these lenses, I viewed the projects of DrScheme and Processing as examples of critical tactical interventions to leverage existing technology and construct new visions of what it means to code on behalf of specific literacy objectives. For Processing, a streamlined interface and programming library transformed the software-professional subject of a general-purpose programming language into the "end user" designer of interactive visualizations more concerned with simple procedural interactions than with complex object-oriented systems. For DrScheme, an incremental programming language aligned with existing educational standards combined existing math and science literacies with basic program design concepts. I take both of these projects to be representative examples of aspects of a critical procedural literacy, not only because they both provide easier and more intuitive environments for individuals to experiment with procedural expression, but because they both radically redefine the associated competencies and finished products of their subjects.

The concept of procedural literacy I have developed in this chapter is sympathetic to \citeauthor{Berry2008}'s prediction that "Introducing democratic accountability to code may well be the democratic challenge of the twenty-first century and steering the implementation of technological artefacts will increasingly contribute to our ability to keep our future open and democratic" \autocite[186]{Berry2008}. As long as we continue to link the notion of literacy to a deliberative ethics of human language and experience, it must be sympathetic to movements that counter, if not actively resist, the culturally and technologically normalizing forces of institutional standardization formed by global flows of systemic power. As one final instance of the latter to which this chapter is aligned, Freire has argued that a critical pedagogy involves "reading the word and the world," developing a transformative critical consciousness through which teacher and student together produce an ever-changing literacy as a practical tool for liberation and social change.\footnote{
  For a comparison of the philosophies of Freire and Habermas, see \autocite{Morrow02}.
} Instead of passively accepting the impact of industry-regulated, market-based network effects on the evolution of notations that comprise our increasingly computerized society, we can imagine a procedural literacy based on democratic principles that advocates for the construction of more public modes of software development for which their mass publics feel collectively responsible.
